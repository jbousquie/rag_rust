//! RAG Proxy Handler Module
//!
//! This module contains the handler function for processing incoming RAG requests.
//! It implements the core logic for the RAG pipeline: extracting the question from
//! the request, retrieving relevant context from Qdrant, and calling the LLM with
//! the combined prompt to generate a response.

use axum::{
    Json,
    body::Bytes,
    http::{HeaderMap, HeaderValue, StatusCode},
    response::IntoResponse,
};
use serde::{Deserialize, Serialize};

use crate::load_config;
use crate::rag_proxy::llm_caller::call_llm_with_messages;
use crate::rag_proxy::retriever::retrieve_context;

/// Chat completion request structure
/// This matches the OpenAI API format for chat completions
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionRequest {
    /// The model to use for the completion
    pub model: String,
    /// The messages in the conversation
    pub messages: Vec<ChatMessage>,
    /// Whether to stream the response
    pub stream: Option<bool>,
}

/// Content of a message - can be either a string or an array of content parts
#[derive(Serialize, Deserialize, Debug, Clone)]
#[serde(untagged)]
pub enum MessageContent {
    /// Simple text content
    Text(String),
    /// Array of content parts (used in multimodal messages)
    Parts(Vec<ContentPart>),
}

/// A part of content in a message
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ContentPart {
    /// Type of the content part
    pub r#type: String,
    /// Text content (only for text type)
    pub text: Option<String>,
}

/// A message in the conversation
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ChatMessage {
    /// The role of the message sender (user, assistant, system)
    pub role: String,
    /// The content of the message
    pub content: MessageContent,
}

/// Chat completion response structure
/// This matches the OpenAI API format for chat completions
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionResponse {
    /// Unique identifier for the response
    pub id: String,
    /// Type of the response object
    pub object: String,
    /// Unix timestamp of when the response was created
    pub created: u64,
    /// The model used for the response
    pub model: String,
    /// The choices generated by the model
    pub choices: Vec<Choice>,
    /// Usage information about the response
    pub usage: Option<Usage>,
}

/// Usage information for the response
#[derive(Serialize, Deserialize, Debug)]
pub struct Usage {
    /// Number of prompt tokens used
    pub prompt_tokens: u32,
    /// Number of completion tokens used
    pub completion_tokens: u32,
    /// Total number of tokens used
    pub total_tokens: u32,
}

/// A choice in the chat completion response
#[derive(Serialize, Deserialize, Debug)]
pub struct Choice {
    /// Index of the choice
    pub index: u32,
    /// The message content
    pub message: ChatMessage,
    /// Reason the response ended
    pub finish_reason: Option<String>,
}

/// Handles incoming RAG requests
///
/// This function processes an incoming chat completion request by:
/// 1. Extracting the user's question from the messages
/// 2. Retrieving relevant context from Qdrant using the question
/// 3. Creating a modified request that extends the existing system message with RAG context
/// 4. Calling the LLM with the modified message structure
/// 5. Returning the LLM's response in OpenAI API compatible format
///
/// # Arguments
/// * `request` - The incoming chat completion request as raw bytes
///
/// # Returns
/// * `impl IntoResponse` - The chat completion response in JSON format
pub async fn handle_rag_request(request: Bytes) -> impl IntoResponse {
    // Debug: Print the raw request to understand the format
    //println!("Raw request received: {:?}", std::str::from_utf8(&request));

    // Parse the request to access its contents
    let parsed_request: Result<ChatCompletionRequest, serde_json::Error> =
        serde_json::from_slice(&request);

    match parsed_request {
        Ok(req) => {
            //println!("Successfully parsed request: {:?}", req);

            // Check if there's already a system message in the incoming request
            let existing_system_messages: Vec<&ChatMessage> = req
                .messages
                .iter()
                .filter(|msg| msg.role == "system")
                .collect();

            if !existing_system_messages.is_empty() {
                println!(
                    "Found {} existing system message(s) in the request:",
                    existing_system_messages.len()
                );
                /*
                for (i, msg) in existing_system_messages.iter().enumerate() {
                    println!("  System message {}: {:?}", i + 1, msg);
                }
                */
            } else {
                println!("No existing system messages found in the request");
            }

            // Extract the user's question from the messages
            let user_question = req
                .messages
                .last() // Use the last message as the user question (most recent user input)
                .or_else(|| req.messages.iter().find(|msg| msg.role == "user")) // Fallback to find any user message
                .map(|msg| {
                    match &msg.content {
                        MessageContent::Text(text) => text.clone(),
                        MessageContent::Parts(parts) => {
                            // For multimodal messages, concatenate all text parts
                            parts
                                .iter()
                                .filter_map(|part| {
                                    if part.r#type == "text" {
                                        part.text.clone()
                                    } else {
                                        None
                                    }
                                })
                                .collect::<Vec<_>>()
                                .join(" ")
                        }
                    }
                })
                .unwrap_or_else(|| "No question provided".to_string());

            // Load configuration
            let config = load_config();

            // Retrieve relevant context from Qdrant
            let context = match retrieve_context(&user_question, &config).await {
                Ok(ctx) => ctx,
                Err(e) => {
                    eprintln!("Error retrieving context: {}", e);
                    return (
                        StatusCode::INTERNAL_SERVER_ERROR,
                        "Failed to retrieve context",
                    )
                        .into_response();
                }
            };

            // Create a new message vector based on the original but with extended system message
            let mut modified_messages = req.messages.clone();

            // Add context if available
            if !context.is_empty() {
                // Format the new context
                let new_context = format!("--- Context from: RAG ---\n{}", context);

                // Find and modify the first system message if one exists
                let mut system_message_updated = false;
                for msg in &mut modified_messages {
                    if msg.role == "system" {
                        if let MessageContent::Text(ref mut text) = msg.content {
                            *text = format!("{}\n\n{}", text, new_context);
                        } else if let MessageContent::Parts(ref mut parts) = msg.content {
                            // For multimodal messages, convert to text and append
                            let text_content = parts
                                .iter()
                                .filter_map(|part| {
                                    if part.r#type == "text" {
                                        part.text.clone()
                                    } else {
                                        None
                                    }
                                })
                                .collect::<Vec<_>>()
                                .join("\n");

                            msg.content = MessageContent::Text(format!(
                                "{}\n\n{}",
                                text_content, new_context
                            ));
                        }
                        system_message_updated = true;
                        break; // Only update the first system message
                    }
                }

                // If no system message existed, add a new one at the beginning
                if !system_message_updated {
                    let context_msg = ChatMessage {
                        role: "system".to_string(),
                        content: MessageContent::Text(new_context),
                    };
                    modified_messages.insert(0, context_msg);
                }
            };

            // Call the LLM with the modified messages
            let response_text =
                match call_llm_with_messages(modified_messages, req.model, &config).await {
                    Ok(resp) => resp,
                    Err(e) => {
                        eprintln!("Error calling LLM: {}", e);
                        return (
                            StatusCode::INTERNAL_SERVER_ERROR,
                            "Failed to get response from LLM",
                        )
                            .into_response();
                    }
                };

            // Create and return the response
            let chat_response = ChatCompletionResponse {
                id: "chatcmpl-123".to_string(),
                object: "chat.completion".to_string(),
                created: std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs(),
                model: config.llm.model.clone(),
                choices: vec![Choice {
                    index: 0,
                    message: ChatMessage {
                        role: "assistant".to_string(),
                        content: MessageContent::Text(response_text),
                    },
                    finish_reason: Some("stop".to_string()),
                }],
                usage: Some(Usage {
                    prompt_tokens: 0,     // Placeholder - actual token count would require a tokenizer
                    completion_tokens: 0, // Placeholder - actual token count would require a tokenizer
                    total_tokens: 0, // Placeholder - actual token count would require a tokenizer
                }),
            };
            //println!("{:?}", chat_response);

            // Create response with proper headers
            let mut headers = HeaderMap::new();
            headers.insert("Content-Type", HeaderValue::from_static("application/json"));

            // Return the response as JSON with headers
            (headers, Json(chat_response)).into_response()
        }
        Err(e) => {
            eprintln!("Failed to parse request: {}", e);
            eprintln!("Raw request content: {:?}", std::str::from_utf8(&request));
            (StatusCode::BAD_REQUEST, "Failed to parse request").into_response()
        }
    }
}
