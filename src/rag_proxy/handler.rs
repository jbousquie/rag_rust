//! RAG Proxy Handler Module
//!
//! This module contains the handler function for processing incoming RAG requests.
//! It implements the core logic for the RAG pipeline: extracting the question from
//! the request, retrieving relevant context from Qdrant, and calling the LLM with
//! the combined prompt to generate a response.

use axum::{
    Json,
    body::Bytes,
    http::{HeaderMap, HeaderValue, StatusCode},
    response::IntoResponse,
};
use serde::{Deserialize, Serialize};

use crate::load_config;
use crate::rag_proxy::llm_caller::call_llm_with_messages;
use crate::rag_proxy::retriever::retrieve_context;

/// Chat completion request structure
/// This matches the OpenAI API format for chat completions
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionRequest {
    /// The model to use for the completion
    pub model: String,
    /// The messages in the conversation
    pub messages: Vec<ChatMessage>,
    /// Whether to stream the response
    pub stream: Option<bool>,
}

/// Content of a message - can be either a string or an array of content parts
#[derive(Serialize, Deserialize, Debug, Clone)]
#[serde(untagged)]
pub enum MessageContent {
    /// Simple text content
    Text(String),
    /// Array of content parts (used in multimodal messages)
    Parts(Vec<ContentPart>),
}

/// A part of content in a message
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ContentPart {
    /// Type of the content part
    pub r#type: String,
    /// Text content (only for text type)
    pub text: Option<String>,
}

/// A message in the conversation
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ChatMessage {
    /// The role of the message sender (user, assistant, system)
    pub role: String,
    /// The content of the message
    pub content: MessageContent,
}

/// Chat completion response structure
/// This matches the OpenAI API format for chat completions
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionResponse {
    /// Unique identifier for the response
    pub id: String,
    /// Type of the response object
    pub object: String,
    /// Unix timestamp of when the response was created
    pub created: u64,
    /// The model used for the response
    pub model: String,
    /// The choices generated by the model
    pub choices: Vec<Choice>,
    /// Usage information about the response
    pub usage: Option<Usage>,
}

/// Usage information for the response
#[derive(Serialize, Deserialize, Debug)]
pub struct Usage {
    /// Number of prompt tokens used
    pub prompt_tokens: u32,
    /// Number of completion tokens used
    pub completion_tokens: u32,
    /// Total number of tokens used
    pub total_tokens: u32,
}

/// A choice in the chat completion response
#[derive(Serialize, Deserialize, Debug)]
pub struct Choice {
    /// Index of the choice
    pub index: u32,
    /// The message content
    pub message: ChatMessage,
    /// Reason the response ended
    pub finish_reason: Option<String>,
}

/// Handles incoming RAG requests
///
/// This function processes an incoming chat completion request by:
/// 1. Extracting the user's question from the messages
/// 2. Retrieving relevant context from Qdrant using the question
/// 3. Creating a modified request that extends the existing system message with RAG context
/// 4. Calling the LLM with the modified message structure
/// 5. Returning the LLM's response in OpenAI API compatible format
///
/// # Arguments
/// * `request` - The incoming chat completion request as raw bytes
///
/// # Returns
/// * `impl IntoResponse` - The chat completion response in JSON format
pub async fn handle_rag_request(request: Bytes) -> impl IntoResponse {
    // Convert bytes to string for JSON manipulation
    let request_str = match std::str::from_utf8(&request) {
        Ok(s) => s,
        Err(e) => {
            eprintln!("Failed to parse request as UTF-8: {}", e);
            return (StatusCode::BAD_REQUEST, "Failed to parse request").into_response();
        }
    };

    // Parse the request to access its contents
    let parsed_request: Result<ChatCompletionRequest, serde_json::Error> =
        serde_json::from_slice(&request);

    let user_question = match parsed_request {
        Ok(req) => {
            // Extract the user's question from the messages
            req.messages
                .last() // Use the last message as the user question (most recent user input)
                .or_else(|| req.messages.iter().find(|msg| msg.role == "user")) // Fallback to find any user message
                .map(|msg| {
                    match &msg.content {
                        MessageContent::Text(text) => text.clone(),
                        MessageContent::Parts(parts) => {
                            // For multimodal messages, concatenate all text parts
                            parts
                                .iter()
                                .filter_map(|part| {
                                    if part.r#type == "text" {
                                        part.text.clone()
                                    } else {
                                        None
                                    }
                                })
                                .collect::<Vec<_>>()
                                .join(" ")
                        }
                    }
                })
                .unwrap_or_else(|| "No question provided".to_string())
        }
        Err(e) => {
            eprintln!("Failed to parse request: {}", e);
            eprintln!("Raw request content: {:?}", std::str::from_utf8(&request));
            return (StatusCode::BAD_REQUEST, "Failed to parse request").into_response();
        }
    };

    // Load configuration
    let config = load_config();

    // Retrieve relevant context from Qdrant
    let context = match retrieve_context(&user_question, &config).await {
        Ok(ctx) => ctx,
        Err(e) => {
            eprintln!("Error retrieving context: {}", e);
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                "Failed to retrieve context",
            )
                .into_response();
        }
    };

    // If we have context, modify the request to include it
    let modified_request_str = if !context.is_empty() {
        // Format the new context
        let new_context = format!("--- Context from: RAG ---\n{}", context);

        // Create a copy of the original request string to preserve structure
        // This approach ensures we maintain the exact JSON structure while only modifying
        // the system message content
        let mut request_json: serde_json::Value = serde_json::from_str(request_str)
            .expect("Failed to parse original request JSON");

        // Find and modify the system message content
        if let Some(messages) = request_json.get_mut("messages") {
            if let Some(messages_array) = messages.as_array_mut() {
                // Look for the last system message to modify
                let system_message_index = messages_array
                    .iter()
                    .enumerate()
                    .rev()
                    .find(|(_, msg)| {
                        msg.get("role").and_then(|r| r.as_str()) == Some("system")
                    })
                    .map(|(i, _)| i);

                if let Some(index) = system_message_index {
                    // Modify the existing system message content
                    if let Some(content) = messages_array[index].get_mut("content") {
                        *content = serde_json::Value::String(new_context);
                    }
                } else {
                    // Add a new system message at the end
                    let system_message = serde_json::json!({
                        "role": "system",
                        "content": new_context
                    });
                    messages_array.push(system_message);
                }
            }
        }

        // Serialize back to string
        request_json.to_string()
    } else {
        request_str.to_string()
    };

    // Parse the modified JSON back to a ChatCompletionRequest
    let modified_request: Result<ChatCompletionRequest, serde_json::Error> =
        serde_json::from_str(&modified_request_str);

    let response_text = match modified_request {
        Ok(req) => {
            // Call the LLM with the modified messages
            match call_llm_with_messages(req.messages, req.model, &config).await {
                Ok(resp) => resp,
                Err(e) => {
                    eprintln!("Error calling LLM: {}", e);
                    return (
                        StatusCode::INTERNAL_SERVER_ERROR,
                        "Failed to get response from LLM",
                    )
                        .into_response();
                }
            }
        }
        Err(e) => {
            eprintln!("Failed to parse modified request: {}", e);
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                "Failed to parse modified request",
            )
                .into_response();
        }
    };

    // Create and return the response
    let chat_response = ChatCompletionResponse {
        id: "chatcmpl-123".to_string(),
        object: "chat.completion".to_string(),
        created: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs(),
        model: config.llm.model.clone(),
        choices: vec![Choice {
            index: 0,
            message: ChatMessage {
                role: "assistant".to_string(),
                content: MessageContent::Text(response_text),
            },
            finish_reason: Some("stop".to_string()),
        }],
        usage: Some(Usage {
            prompt_tokens: 0,     // Placeholder - actual token count would require a tokenizer
            completion_tokens: 0, // Placeholder - actual token count would require a tokenizer
            total_tokens: 0, // Placeholder - actual token count would require a tokenizer
        }),
    };

    // Create response with proper headers
    let mut headers = HeaderMap::new();
    headers.insert("Content-Type", HeaderValue::from_static("application/json"));

    // Return the response as JSON with headers
    (headers, Json(chat_response)).into_response()
}
