//! RAG Proxy Handler Module
//!
//! This module contains the handler function for processing incoming RAG requests.
//! It implements the core logic for the RAG pipeline: extracting the question from
//! the request, retrieving relevant context from Qdrant, and calling the LLM with
//! the combined prompt to generate a response.

use axum::{Json, http::StatusCode, response::IntoResponse};
use serde::{Deserialize, Serialize};

use crate::load_config;
use crate::rag_proxy::llm_caller::call_llm;
use crate::rag_proxy::retriever::retrieve_context;

/// Chat completion request structure
/// This matches the OpenAI API format for chat completions
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionRequest {
    /// The model to use for the completion
    pub model: String,
    /// The messages in the conversation
    pub messages: Vec<ChatMessage>,
    /// Whether to stream the response
    pub stream: Option<bool>,
}

/// A message in the conversation
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatMessage {
    /// The role of the message sender (user, assistant, system)
    pub role: String,
    /// The content of the message
    pub content: String,
}

/// Chat completion response structure
/// This matches the OpenAI API format for chat completions
#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionResponse {
    /// Unique identifier for the response
    pub id: String,
    /// Type of the response object
    pub object: String,
    /// Unix timestamp of when the response was created
    pub created: u64,
    /// The model used for the response
    pub model: String,
    /// The choices generated by the model
    pub choices: Vec<Choice>,
}

/// A choice in the chat completion response
#[derive(Serialize, Deserialize, Debug)]
pub struct Choice {
    /// Index of the choice
    pub index: u32,
    /// The message content
    pub message: ChatMessage,
    /// Reason the response ended
    pub finish_reason: Option<String>,
}

/// Handles incoming RAG requests
///
/// This function processes an incoming chat completion request by:
/// 1. Extracting the user's question from the messages
/// 2. Retrieving relevant context from Qdrant using the question
/// 3. Constructing a prompt with the context and question
/// 4. Calling the LLM with the prompt
/// 5. Returning the LLM's response in OpenAI API compatible format
///
/// # Arguments
/// * `Json(request)` - The incoming chat completion request
///
/// # Returns
/// * `impl IntoResponse` - The chat completion response in JSON format
pub async fn handle_rag_request(Json(request): Json<ChatCompletionRequest>) -> impl IntoResponse {
    // Extract the user's question from the messages
    let user_question = request
        .messages
        .iter()
        .find(|msg| msg.role == "user")
        .map(|msg| msg.content.clone())
        .unwrap_or_else(|| "No question provided".to_string());

    // Load configuration
    let config = load_config();

    // Retrieve relevant context from Qdrant
    let context = match retrieve_context(&user_question, &config).await {
        Ok(ctx) => ctx,
        Err(e) => {
            eprintln!("Error retrieving context: {}", e);
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                "Failed to retrieve context",
            )
                .into_response();
        }
    };

    // Construct the prompt with context
    let prompt = format!(
        "Context: {}\n\nQuestion: {}\n\nAnswer:",
        context, user_question
    );

    // Call the LLM with the constructed prompt
    let response = match call_llm(&prompt, &config).await {
        Ok(resp) => resp,
        Err(e) => {
            eprintln!("Error calling LLM: {}", e);
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                "Failed to get response from LLM",
            )
                .into_response();
        }
    };

    // Create and return the response
    let chat_response = ChatCompletionResponse {
        id: "chatcmpl-123".to_string(),
        object: "chat.completion".to_string(),
        created: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs(),
        model: config.llm.model.clone(),
        choices: vec![Choice {
            index: 0,
            message: ChatMessage {
                role: "assistant".to_string(),
                content: response,
            },
            finish_reason: Some("stop".to_string()),
        }],
    };

    // Return the response as JSON
    Json(chat_response).into_response()
}
